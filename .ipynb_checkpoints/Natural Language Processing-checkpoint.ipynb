{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eca13113",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7edbc70",
   "metadata": {},
   "source": [
    "- **Basic Steps involved in NLP with the help of Natural Language Tool Kit (nltk) as follow**:\n",
    "\n",
    "> 1. Tokenization.\n",
    "> 2. Stop Word Exclusion.\n",
    "> 3. Stemming / Lemmatization.\n",
    "> 4. POS Tagging (Part-of-Speech)\n",
    "> 5. Chunking (using Regular Expressions & RegexParser)\n",
    "> 6. NER (Named Entity Recognition) - kind of Forming groups of similar kinds `ne_chunk()`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886f6abf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35bdc32",
   "metadata": {},
   "source": [
    "## NLTK Installation:\n",
    "\n",
    "`pip install nltk`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9f342",
   "metadata": {},
   "source": [
    "## Importing Necessary Dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d89d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# Natural Language Tool kit\n",
    "import nltk\n",
    "# nltk.download('package_name') - for installation of nltk packages\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de5c4c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4760d67",
   "metadata": {},
   "source": [
    "## 1. TOKENIZATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6c6e49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91800\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06da1f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'bought', 'these', 'for', 'my', 'husband', 'and', 'he', 'said', 'they', 'are', 'the', 'best', 'energy', 'shots', 'out', 'there', '.', 'He', 'takes', 'one', 'in', 'the', 'mornings', 'and', 'works', 'hard', 'all', 'day', '.', 'Good', 'stuff', '!']\n"
     ]
    }
   ],
   "source": [
    "testText_01 = 'I bought these for my husband and he said they are the best energy shots out there. He takes one in the mornings and works hard all day. Good stuff!'\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(testText_01)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34954471",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e196ab8",
   "metadata": {},
   "source": [
    "## 2. Stop Word Exclusion: \n",
    "- Stop Word includes the word that do not add that much meaning to the sentence. Eg.: 'a', 'an', 'the', 'and', etc.\n",
    "- package used stopwords for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cd35ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91800\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aa8410c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous: \n",
      " ['I', 'bought', 'these', 'for', 'my', 'husband', 'and', 'he', 'said', 'they', 'are', 'the', 'best', 'energy', 'shots', 'out', 'there', '.', 'He', 'takes', 'one', 'in', 'the', 'mornings', 'and', 'works', 'hard', 'all', 'day', '.', 'Good', 'stuff', '!']\n",
      "\n",
      "Filtered: \n",
      " ['bought', 'husband', 'said', 'best', 'energy', 'shots', '.', 'takes', 'one', 'mornings', 'works', 'hard', 'day', '.', 'Good', 'stuff', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# print(stop_words)\n",
    "\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "print(\"Previous: \\n\", tokens)\n",
    "print(\"\\nFiltered: \\n\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca1f8f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24817979",
   "metadata": {},
   "source": [
    "## 3. Stemming / Lemmatization:\n",
    "> **Stemming:** Used in large datasets, eg: `Caring -> Car` (can lead to incorrect meaning also).\n",
    "\n",
    "> **Lemmatization:** Converts the word to its meaningful base (Lemma), eg: `Caring -> Care` (expensive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7671dd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91800\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Package download\n",
    "nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "350f4d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'bought', 'these', 'for', 'my', 'husband', 'and', 'he', 'said', 'they', 'are', 'the', 'best', 'energy', 'shots', 'out', 'there', '.', 'He', 'takes', 'one', 'in', 'the', 'mornings', 'and', 'works', 'hard', 'all', 'day', '.', 'Good', 'stuff', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a12c3555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemming:\n",
      " \t ['i', 'bought', 'these', 'for', 'my', 'husband', 'and', 'he', 'said', 'they', 'are', 'the', 'best', 'energi', 'shot', 'out', 'there', '.', 'he', 'take', 'one', 'in', 'the', 'morn', 'and', 'work', 'hard', 'all', 'day', '.', 'good', 'stuff', '!']\n",
      "\n",
      "\n",
      "Snowball Stemming:\n",
      " \t ['i', 'bought', 'these', 'for', 'my', 'husband', 'and', 'he', 'said', 'they', 'are', 'the', 'best', 'energi', 'shot', 'out', 'there', '.', 'he', 'take', 'one', 'in', 'the', 'morn', 'and', 'work', 'hard', 'all', 'day', '.', 'good', 'stuff', '!']\n"
     ]
    }
   ],
   "source": [
    "# Stemming:\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "stemmed_tokens_porter = [porter_stemmer.stem(token) for token in tokens]\n",
    "print('Porter Stemming:\\n \\t',stemmed_tokens_porter)\n",
    "\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "stemmed_tokens_snowball = [snowball_stemmer.stem(token) for token in tokens]\n",
    "print('\\n\\nSnowball Stemming:\\n \\t', stemmed_tokens_snowball)\n",
    "\n",
    "# Lemmatization:\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "#print('\\n\\nLemmatization:\\n \\t', lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2614f78",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b693a9",
   "metadata": {},
   "source": [
    "## 4. POS Tagging (Part-Of-Speech): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87ce32df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\91800\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dee444b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('bought', 'VBD'), ('these', 'DT'), ('for', 'IN'), ('my', 'PRP$'), ('husband', 'NN'), ('and', 'CC'), ('he', 'PRP'), ('said', 'VBD'), ('they', 'PRP'), ('are', 'VBP'), ('the', 'DT'), ('best', 'JJS'), ('energy', 'NN'), ('shots', 'NNS'), ('out', 'RP'), ('there', 'RB'), ('.', '.'), ('He', 'PRP'), ('takes', 'VBZ'), ('one', 'CD'), ('in', 'IN'), ('the', 'DT'), ('mornings', 'NNS'), ('and', 'CC'), ('works', 'VBZ'), ('hard', 'JJ'), ('all', 'DT'), ('day', 'NN'), ('.', '.'), ('Good', 'JJ'), ('stuff', 'NN'), ('!', '.')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('bought', 'VBD'),\n",
       " ('these', 'DT'),\n",
       " ('for', 'IN'),\n",
       " ('my', 'PRP$')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taggings = nltk.pos_tag(tokens)\n",
    "print(taggings)\n",
    "taggings[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab05e5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6631748",
   "metadata": {},
   "source": [
    "## 5. Chunking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9a86e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
