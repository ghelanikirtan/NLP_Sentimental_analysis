{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "448e206b",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0591f8cd",
   "metadata": {},
   "source": [
    "- **Basic Steps involved in NLP with the help of Natural Language Tool Kit (nltk) as follow**:\n",
    "\n",
    "> 1. Tokenization.\n",
    "> 2. Stop Word Exclusion.\n",
    "> 3. Stemming / Lemmatization.\n",
    "> 4. POS Tagging (Part-of-Speech)\n",
    "> 5. Chunking (using Regular Expressions & RegexParser)\n",
    "> 6. NER (Named Entity Recognition) - kind of Forming groups of similar kinds `ne_chunk()`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aa7cf9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e5f5c7",
   "metadata": {},
   "source": [
    "## NLTK Installation:\n",
    "\n",
    "`pip install nltk`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bb03cb",
   "metadata": {},
   "source": [
    "## Importing Necessary Dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5b54e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# Natural Language Tool kit\n",
    "import nltk\n",
    "# nltk.download('package_name') - for installation of nltk packages\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d5bd43",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c1015",
   "metadata": {},
   "source": [
    "## 1. TOKENIZATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b07e6c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91800\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "288a3816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'bought', 'these', 'for', 'my', 'husband', 'and', 'he', 'said', 'they', 'are', 'the', 'best', 'energy', 'shots', 'out', 'there', '.', 'He', 'takes', 'one', 'in', 'the', 'mornings', 'and', 'works', 'hard', 'all', 'day', '.', 'Good', 'stuff', '!']\n"
     ]
    }
   ],
   "source": [
    "testText_01 = 'I bought these for my husband and he said they are the best energy shots out there. He takes one in the mornings and works hard all day. Good stuff!'\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(testText_01)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6fa34a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07965a52",
   "metadata": {},
   "source": [
    "## 2. Stop Word Exclusion: \n",
    "- Stop Word includes the word that do not add that much meaning to the sentence. Eg.: 'a', 'an', 'the', 'and', etc.\n",
    "- package used stopwords for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abbe19a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91800\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf9c02dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous: \n",
      " ['I', 'bought', 'these', 'for', 'my', 'husband', 'and', 'he', 'said', 'they', 'are', 'the', 'best', 'energy', 'shots', 'out', 'there', '.', 'He', 'takes', 'one', 'in', 'the', 'mornings', 'and', 'works', 'hard', 'all', 'day', '.', 'Good', 'stuff', '!']\n",
      "\n",
      "Filtered: \n",
      " ['bought', 'husband', 'said', 'best', 'energy', 'shots', '.', 'takes', 'one', 'mornings', 'works', 'hard', 'day', '.', 'Good', 'stuff', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# print(stop_words)\n",
    "\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "print(\"Previous: \\n\", tokens)\n",
    "print(\"\\nFiltered: \\n\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92237d22",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34d2c2",
   "metadata": {},
   "source": [
    "## 3. Stemming / Lemmatization:\n",
    "> **Stemming:** Used in large datasets, eg: `Caring -> Car` (can lead to incorrect meaning also).\n",
    "\n",
    "> **Lemmatization:** Converts the word to its meaningful base (Lemma), eg: `Caring -> Care` (expensive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a8d01a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91800\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Package download\n",
    "nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "352e7a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'bought', 'these', 'for', 'my', 'husband', 'and', 'he', 'said', 'they', 'are', 'the', 'best', 'energy', 'shots', 'out', 'there', '.', 'He', 'takes', 'one', 'in', 'the', 'mornings', 'and', 'works', 'hard', 'all', 'day', '.', 'Good', 'stuff', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb0cd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemming:\n",
      " \t ['i', 'bought', 'these', 'for', 'my', 'husband', 'and', 'he', 'said', 'they', 'are', 'the', 'best', 'energi', 'shot', 'out', 'there', '.', 'he', 'take', 'one', 'in', 'the', 'morn', 'and', 'work', 'hard', 'all', 'day', '.', 'good', 'stuff', '!']\n",
      "\n",
      "\n",
      "Snowball Stemming:\n",
      " \t ['i', 'bought', 'these', 'for', 'my', 'husband', 'and', 'he', 'said', 'they', 'are', 'the', 'best', 'energi', 'shot', 'out', 'there', '.', 'he', 'take', 'one', 'in', 'the', 'morn', 'and', 'work', 'hard', 'all', 'day', '.', 'good', 'stuff', '!']\n"
     ]
    }
   ],
   "source": [
    "# Stemming:\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "stemmed_tokens_porter = [porter_stemmer.stem(token) for token in tokens]\n",
    "print('Porter Stemming:\\n \\t',stemmed_tokens_porter)\n",
    "\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "stemmed_tokens_snowball = [snowball_stemmer.stem(token) for token in tokens]\n",
    "print('\\n\\nSnowball Stemming:\\n \\t', stemmed_tokens_snowball)\n",
    "\n",
    "# Lemmatization:\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "#print('\\n\\nLemmatization:\\n \\t', lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd1be5e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca5ebc",
   "metadata": {},
   "source": [
    "## 4. POS Tagging (Part-Of-Speech): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a94c5533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\91800\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30dd62df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('bought', 'VBD'), ('these', 'DT'), ('for', 'IN'), ('my', 'PRP$'), ('husband', 'NN'), ('and', 'CC'), ('he', 'PRP'), ('said', 'VBD'), ('they', 'PRP'), ('are', 'VBP'), ('the', 'DT'), ('best', 'JJS'), ('energy', 'NN'), ('shots', 'NNS'), ('out', 'RP'), ('there', 'RB'), ('.', '.'), ('He', 'PRP'), ('takes', 'VBZ'), ('one', 'CD'), ('in', 'IN'), ('the', 'DT'), ('mornings', 'NNS'), ('and', 'CC'), ('works', 'VBZ'), ('hard', 'JJ'), ('all', 'DT'), ('day', 'NN'), ('.', '.'), ('Good', 'JJ'), ('stuff', 'NN'), ('!', '.')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('bought', 'VBD'),\n",
       " ('these', 'DT'),\n",
       " ('for', 'IN'),\n",
       " ('my', 'PRP$')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taggings = nltk.pos_tag(tokens)\n",
    "print(taggings)\n",
    "taggings[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50077a20",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba3c9e9",
   "metadata": {},
   "source": [
    "## 5. Chunking:\n",
    "- In this Grouping of words is done into `chunks` based on the Part-of-Speech.\n",
    "- Chunking can be performed using Regular Expressions and the `RegexParser` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4a78c17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\91800\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab3e1d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('bought', 'VBD'),\n",
       " ('these', 'DT'),\n",
       " ('for', 'IN'),\n",
       " ('my', 'PRP$')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taggings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05e3f0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  bought/VBD\n",
      "  these/DT\n",
      "  for/IN\n",
      "  my/PRP$\n",
      "  (NP husband/NN)\n",
      "  and/CC\n",
      "  he/PRP\n",
      "  said/VBD\n",
      "  they/PRP\n",
      "  are/VBP\n",
      "  the/DT\n",
      "  best/JJS\n",
      "  (NP energy/NN)\n",
      "  shots/NNS\n",
      "  out/RP\n",
      "  there/RB\n",
      "  ./.\n",
      "  He/PRP\n",
      "  takes/VBZ\n",
      "  one/CD\n",
      "  in/IN\n",
      "  the/DT\n",
      "  mornings/NNS\n",
      "  and/CC\n",
      "  works/VBZ\n",
      "  hard/JJ\n",
      "  (NP all/DT day/NN)\n",
      "  ./.\n",
      "  (NP Good/JJ stuff/NN)\n",
      "  !/.)\n"
     ]
    }
   ],
   "source": [
    "chunk_parser = nltk.RegexpParser(r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>} # chunk determiner/adj+noun\n",
    "    PP: {<IN><NP>} # chunk preposition+NP\n",
    "    VP: {<VB.*><NP|PP|CLAUSE>+$} # chunk verbs and their arguments\n",
    "    CLAUSE: {<NP><VP>} # chunk NP, VP\n",
    "\"\"\")\n",
    "\n",
    "# groups:\n",
    "chunks = chunk_parser.parse(taggings)\n",
    "#print(chunks)\n",
    "chunks.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3480d8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd148451",
   "metadata": {},
   "source": [
    "## 6. NER (Named Entity Recognition):\n",
    "> Classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. \n",
    "\n",
    "> **[🔗NER](https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da)** is used in many fields in Natural Language Processing (NLP), and it can help answering many real-world questions, such as:\n",
    ">- Which companies were mentioned in the news article?\n",
    ">- Were specified products mentioned in complaints or reviews?\n",
    ">- Does the tweet contain the name of a person? Does the tweet contain this person’s location?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b7b76c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\91800\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99b697d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  bought/VBD\n",
      "  these/DT\n",
      "  for/IN\n",
      "  my/PRP$\n",
      "  husband/NN\n",
      "  and/CC\n",
      "  he/PRP\n",
      "  said/VBD\n",
      "  they/PRP\n",
      "  are/VBP\n",
      "  the/DT\n",
      "  best/JJS\n",
      "  energy/NN\n",
      "  shots/NNS\n",
      "  out/RP\n",
      "  there/RB\n",
      "  ./.\n",
      "  He/PRP\n",
      "  takes/VBZ\n",
      "  one/CD\n",
      "  in/IN\n",
      "  the/DT\n",
      "  mornings/NNS\n",
      "  and/CC\n",
      "  works/VBZ\n",
      "  hard/JJ\n",
      "  all/DT\n",
      "  day/NN\n",
      "  ./.\n",
      "  Good/JJ\n",
      "  stuff/NN\n",
      "  !/.)\n"
     ]
    }
   ],
   "source": [
    "entities = nltk.ne_chunk(taggings)\n",
    "entities.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f2ff66",
   "metadata": {},
   "source": [
    "---\n",
    "### Above were the basics steps to be consider for NLP ✨\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b320fed",
   "metadata": {},
   "source": [
    "## 7. Sentiment Analysis:\n",
    "\n",
    "> Here we will use VADER (A trained model for Sentiment Intensity Analyzing) by NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5794dad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\91800\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VADER - Package Installation\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d0a24ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Text:\n",
      "\t I bought these for my husband and he said they are the best energy shots out there. He takes one in the mornings and works hard all day. Good stuff!\n",
      "\n",
      "Scores:\n",
      " {'neg': 0.039, 'neu': 0.697, 'pos': 0.264, 'compound': 0.8439}\n"
     ]
    }
   ],
   "source": [
    "# Importing model\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "print('Test Text:\\n\\t', testText_01)\n",
    "\n",
    "# Loading model (Already Trained)\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Measuring Scores\n",
    "scores = analyzer.polarity_scores(testText_01)\n",
    "\n",
    "print(\"\\nScores:\\n\",scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1e842",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c18b4",
   "metadata": {},
   "source": [
    "### Refer for more in-depth details about NLP : [NLP Documentations 🔗](https://realpython.com/nltk-nlp-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
